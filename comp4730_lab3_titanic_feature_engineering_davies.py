# -*- coding: utf-8 -*-
"""COMP4730_Lab3_Titanic_Feature_Engineering_Davies.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19O8G5d3bbyC8zr44D4HRJQyieNZ1VJXG

> **STUDENT STARTER VERSION**  
This notebook contains **TODO** markers where you must write code or add brief explanations.  
Please keep your outputs visible before exporting to PDF.

# COMP 4730 — Machine Learning  
## Lab 3: Feature Engineering & Selection with the Titanic Dataset

**Institution:** School of Computer Science — University of Windsor  
**Course:** COMP 4730  
**Instructor:** Dr. Sherif Saad  
**Student:** Matt Davies • 110121897

---

### Learning Objectives
- Load and explore a real-world dataset (Titanic).
- Handle missing data and encode categorical features.
- Engineer new features and reason about their utility.
- Apply simple feature selection methods (Filter, Wrapper, Embedded).
- Evaluate models before/after feature engineering.

> **Submission:** Upload this notebook (.ipynb) **and** a PDF export to Brightspace.

### Responsible Use of AI Tools (Disclosure Required)

You may use AI tools (ChatGPT, Copilot, Gemini) **for learning support** (error explanations, hints, clarifications).  
If used, add a short note in the **AI-Assistance Disclosure** section at the end describing:
- Which tool you used
- What you asked for
- What part is your own work

_Not acceptable:_ turning in code you do not understand or cannot explain.
"""

# @title Setup
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_selection import mutual_info_classif, RFE
from sklearn.ensemble import RandomForestClassifier

sns.set()
print("Libraries imported.")

"""
## Part 1 — Load & Inspect the Titanic Dataset (20%)

**Tasks**
1. Load dataset using `seaborn.load_dataset("titanic")` into `df`.
2. Inspect shape, `head()`, `info()`, `describe(include="all")`.
3. Identify columns with missing values and types (continuous vs categorical).
4. Select initial working columns:  
   `pclass, sex, age, sibsp, parch, fare, embarked, survived`

_Add short notes on which features you expect to matter for survival and why._
#T1 and T2 below:




"""

# TODO: Load the Titanic dataset
df = sns.load_dataset("titanic")

print("Shape:", df.shape)
display(df.head())
print(df)

# TODO: Inspect structure and missing values
display(df.info())
display(df.describe(include="all"))
print("\nMissing values per column:")
print(df.isna().sum())

# TODO: Create a working copy with selected columns
target = "survived"
features = ["pclass","sex","age","sibsp","parch","fare","embarked"]
data = df[features + [target]].copy()
display(data.head())

"""# The dataset contains missing values in three columns:
- age has 177 missing values (continuous),
- embarked has 2 missing values (categorical),
- deck has 688 missing values (categorical but mostly missing).

Age and fare are continuous numerical features, while pclass, sex, sibsp, parch, and embarked are categorical. Deck has too many missing values to be reliable and will be dropped later.
# T4: Notes on Initial Feature Selection

4.Select initial working columns:
pclass, sex, age, sibsp, parch, fare, embarked, survived

I selected the columns pclass, sex, age, sibsp, parch, fare, embarked, and survived as my initial working features.

Based on intuition and historical information, I expect the following features to matter for survival:

- **sex**: Women were prioritized during evacuation, so sex is a strong predictor.
- **age**: Children were also prioritized, so younger passengers likely had higher survival rates.
- **pclass**: Higher-class passengers (1st class) had better access to lifeboats.
- **fare**: Fare is related to socioeconomic status and may correlate with survival.
- **sibsp** and **parch**: Traveling with family members can influence survival, either positively (helping each other) or negatively (slowed down during evacuation).
- **embarked**: Different ports had passengers in different deck locations, which could affect evacuation access.

The target variable is **survived**, which indicates whether a passenger lived or not.

## Part 2 — Cleaning & Encoding (35% total; 20% cleaning + 15% encoding)

**Cleaning (20%)**
- Impute continuous: `age`, `fare` with **median**.
- Impute categorical: `embarked` with **mode**.
- Briefly justify imputation choices in a markdown cell.

**Encoding (15%)**
- One-hot encode `sex`, `embarked` with `pd.get_dummies(..., drop_first=True)`.
- Show resulting columns and briefly explain dummy variables.
"""

# TODO: Handle missing values
print("Before imputation:\n", data.isna().sum())

data["age"] = data["age"].fillna(data["age"].median())
data["fare"] = data["fare"].fillna(data["fare"].median())
data["embarked"] = data["embarked"].fillna(data["embarked"].mode()[0])

print("\nAfter imputation:\n", data.isna().sum())
display(data.head())

"""### T4. Imputation Justification

For the continuous features **age** and **fare**, I used the median. Both of these
columns contain skewed values, and the median is more robust to outliers than the
mean. This prevents extreme values from heavily influencing the imputed results.

For the categorical feature **embarked**, I used the mode (most common category).
Since only two entries were missing, filling them with the most frequent value keeps
the distribution of the column consistent and avoids introducing artificial categories.

"""

# TODO: One-hot encode categorical features
cat_cols = ["sex","embarked"]
data_encoded = pd.get_dummies(data, columns=cat_cols, drop_first=True)
print("Encoded columns:", list(data_encoded.columns))
display(data_encoded.head())

"""### T5 and T6. Explanation of Dummy Variables

One-hot encoding converted the categorical features into numeric binary columns:

- **sex_male**: 1 if the passenger is male, 0 if female (female is the dropped category).
- **embarked_Q**: 1 if the passenger embarked at Queenstown, 0 otherwise.
- **embarked_S**: 1 if the passenger embarked at Southampton, 0 otherwise.
  
The categories dropped due to `drop_first=True` are:
- sex_female
- embarked_C

Dropping one category avoids multicollinearity and keeps the model interpretable.  
These dummy variables allow logistic regression to incorporate categorical
features without imposing an artificial ordering.

## Part 3 — Feature Engineering (25%)

Create **at least two** new features, suggested:
- `family_size = sibsp + parch + 1`
- `is_alone = 1 if family_size == 1 else 0`
- `fare_per_person = fare / family_size`

_Add 1–3 lines each explaining why the feature may help predict survival._
"""

# TODO: Create engineered features
data_fe = data_encoded.copy()
data_fe["family_size"] = data_fe["sibsp"] + data_fe["parch"] + 1
data_fe["is_alone"] = (data_fe["family_size"] == 1).astype(int)
data_fe["fare_per_person"] = data_fe["fare"] / data_fe["family_size"]
display(data_fe.head())

"""### T7. and T8. Feature Engineering Explanation

**family_size**: This feature combines the number of siblings/spouses and parents/children aboard. Larger family groups may have had a higher chance of helping each other during evacuation, while very small or very large groups might behave differently in survival patterns.

**is_alone**: Passengers traveling alone may have had fewer people to help them or rely on during the evacuation process. This feature captures whether social support (family presence) influenced survival odds.

**fare_per_person**: Dividing fare by family size gives a better sense of fare relative to group size, which can reflect economic status more accurately. This can help the model distinguish between wealthy passengers traveling alone versus groups sharing a single fare.

## Part 4 — Train/Test Split, Scaling, and Baselines (25%)

1. Split (80/20) with `stratify=y`.
2. Scale continuous features with `StandardScaler`:
   - Baseline: likely `age`, `fare`
   - With FE: `age`, `fare`, `family_size`, `fare_per_person`
3. Baseline model: Logistic Regression on encoded data **without** engineered features.
4. FE model: Logistic Regression on **engineered** feature set.
5. Compare metrics (accuracy + report); comment on changes.
"""

# Baseline (no engineered features)
data_baseline = data_encoded.copy()
y_base = data_baseline[target]
X_base = data_baseline.drop(columns=[target])

Xb_train, Xb_test, yb_train, yb_test = train_test_split(
    X_base, y_base, test_size=0.2, random_state=42, stratify=y_base
)

continuous_cols_base = ["age","fare"]
scaler_base = StandardScaler()
Xb_train[continuous_cols_base] = scaler_base.fit_transform(Xb_train[continuous_cols_base])
Xb_test[continuous_cols_base] = scaler_base.transform(Xb_test[continuous_cols_base])

log_reg_base = LogisticRegression(max_iter=1000)
log_reg_base.fit(Xb_train, yb_train)
yb_pred = log_reg_base.predict(Xb_test)

print("Baseline accuracy:", accuracy_score(yb_test, yb_pred))
print("\nClassification report (baseline):\n", classification_report(yb_test, yb_pred))
print("\nConfusion matrix (baseline):\n", confusion_matrix(yb_test, yb_pred))

"""### T9, T10, T11: Baseline Model Evaluation (No Engineered Features)

The baseline logistic regression model achieved an accuracy of about 0.80.
The model performs better at predicting non-survivors (class 0) than survivors
(class 1), which is expected due to class imbalance. The confusion matrix shows
that the model correctly identifies most non-survivors but misses some positive
cases (survivors). This provides a reference point for evaluating whether feature
engineering leads to improvement.

"""

# Feature-Engineered model
y = data_fe[target]
X = data_fe.drop(columns=[target])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

continuous_cols = ["age","fare","family_size","fare_per_person"]
scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()
X_train_scaled[continuous_cols] = scaler.fit_transform(X_train[continuous_cols])
X_test_scaled[continuous_cols] = scaler.transform(X_test[continuous_cols])

log_reg_fe = LogisticRegression(max_iter=1000)
log_reg_fe.fit(X_train_scaled, y_train)
y_pred = log_reg_fe.predict(X_test_scaled)

print("Feature-engineered accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification report (FE):\n", classification_report(y_test, y_pred))
print("\nConfusion matrix (FE):\n", confusion_matrix(y_test, y_pred))

"""### T12. and T13. Feature-Engineered Model Evaluation

After adding the engineered features (family_size, is_alone, fare_per_person),
the accuracy remained roughly the same as the baseline (≈0.80). However, the
recall and F1-score for the survivor class (class 1) improved slightly. This
suggests that the engineered features helped the model capture additional patterns
related to survival, even if overall accuracy did not change much.

The engineered model makes fewer mistakes on survivors compared to the baseline,
which indicates an improvement in identifying the minority class.

## Part 5 — Simple Feature Selection (10%)

Use **one** method to rank features and try a reduced model:
- Filter: `mutual_info_classif`
- Wrapper: `RFE` with LogisticRegression
- Embedded: `SelectFromModel` with RandomForestClassifier (optional)

Train with the **top 8** features; compare accuracy vs. full model.
##T14. and T15 below:
"""

# Mutual Information (Filter)
mi = mutual_info_classif(X_train_scaled, y_train, random_state=42)
mi_series = pd.Series(mi, index=X_train_scaled.columns).sort_values(ascending=False)
print("Top 15 features by MI:")
display(mi_series.head(15))

plt.figure(figsize=(8,6))
mi_series.head(15).plot(kind="barh")
plt.gca().invert_yaxis()
plt.title("Top features by Mutual Information")
plt.show()

# RFE (Wrapper) with Logistic Regression
rfe_estimator = LogisticRegression(max_iter=1000)
rfe = RFE(rfe_estimator, n_features_to_select=8)
rfe.fit(X_train_scaled, y_train)

selected_features = X_train_scaled.columns[rfe.support_]
print("Selected by RFE:", list(selected_features))

X_train_sel = X_train_scaled[selected_features]
X_test_sel = X_test_scaled[selected_features]

log_reg_sel = LogisticRegression(max_iter=1000)
log_reg_sel.fit(X_train_sel, y_train)
y_sel_pred = log_reg_sel.predict(X_test_sel)

print("Selected-feature accuracy:", accuracy_score(y_test, y_sel_pred))
print("\nClassification report (Selected):\n", classification_report(y_test, y_sel_pred))

"""### T16. Part 5: Feature Selection Analysis

I used Recursive Feature Elimination (RFE) with Logistic Regression to select
the top 8 features. RFE works by repeatedly fitting the model and removing the
least important features until the desired number remains. The selected features
were:

`pclass, age, sibsp, sex_male, embarked_Q, embarked_S, family_size, fare_per_person`

After retraining the model using only these features, the accuracy (≈0.804) was
essentially the same as the full feature-engineered model. The precision, recall,
and F1-scores also remained very similar.

This indicates that the model does not require all available features to achieve
good performance. Using fewer features reduces model complexity, speeds up
training, and improves interpretability without sacrificing accuracy. In this case,
RFE identified a compact set of features that capture most of the predictive power
in the dataset.

---

## Reflection (5%)

Answer briefly (3–6 lines each):
1. Which engineered feature(s) had the greatest impact? Why?
2. Did feature selection help performance or interpretability? Explain.
3. If you had more raw data, what new features would you create?

## AI-Assistance Disclosure (if applicable)

- Tool(s) used:  
- What I asked for:  
- How it helped:  
- What remains my own work:

## Appendix (Optional)

Add any extra experiments here:
- Polynomial features
- RobustScaler vs StandardScaler
- RandomForest feature importances

### Reflection

**1. Which engineered feature(s) had the greatest impact? Why?**  
Among the engineered features, *fare_per_person* had the strongest impact because it captures economic status more accurately than raw fare alone. It adjusts for family size and helps distinguish wealthy individuals traveling with others versus individuals sharing a lower fare. *Family_size* and *is_alone* also contributed by capturing social structure, which influenced evacuation patterns.

**2. Did feature selection help performance or interpretability? Explain.**  
Feature selection did not significantly change accuracy, but it did improve interpretability. By reducing the model to the top eight features, it becomes clearer which variables truly matter for predicting survival. The selected feature set captures most of the predictive power while simplifying the model, showing that many remaining features were redundant.

**3. If you had more raw data, what new features would you create?**  
With more data, I would create features related to cabin location, passenger deck, ticket group sizes, and travel purpose (e.g., families vs solo travelers). I would also include interaction terms such as *age x sex* or *pclass x fare* to capture more realistic survival patterns. Additional geographic or socioeconomic data could further improve predictions.

### AI-Assistance Disclosure

- **Tool(s) used:** ChatGPT  
- **What I asked for:** Clarification on parts of the lab and guidance on interpreting model results.  
- **How it helped:** Provided explanations and and improved clarity for reflection and documentation sections.  
- **What remains my own work:** Analysis of all of the following: coding, data cleaning, feature engineering, model training, interpretation of results, and final decisions, and reflection.
"""